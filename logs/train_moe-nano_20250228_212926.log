/Users/dino/Documents/Projects/ParrotLM/src/train.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler()  # For mixed precision training
/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
Mixed precision training enabled for memory efficiency
Overriding batch size to 8
Gradient checkpointing enabled for memory efficiency
  0%|          | 0/100000 [00:00<?, ?it/s]CPU Memory: 0.51 GB used, Peak: 0.51 GB
Memory monitoring started
/Users/dino/Documents/Projects/ParrotLM/src/train.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=hasattr(self.config, 'use_mixed_precision') and self.config.use_mixed_precision):
/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling
  warnings.warn(
/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  0%|          | 0/100000 [00:17<?, ?it/s]
Memory monitoring stopped
Peak GPU memory: 0.00 GB
Peak CPU memory: 0.51 GB
Potential memory leaks detected:
  - Not enough data to detect leaks
Traceback (most recent call last):
  File "/Users/dino/Documents/Projects/ParrotLM/src/train.py", line 380, in <module>
    main()
  File "/Users/dino/Documents/Projects/ParrotLM/src/train.py", line 377, in main
    trainer.train()
  File "/Users/dino/Documents/Projects/ParrotLM/src/train.py", line 163, in train
    loss = self.train_step(batch)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/src/train.py", line 107, in train_step
    self.scaler.scale(scaled_loss).backward()
  File "/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 304, in backward
    outputs = ctx.run_function(*detached_inputs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/src/models/gpt.py", line 81, in custom_forward
    return module(*inputs)
           ^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/src/models/block.py", line 228, in forward
    x = self._attention_layer(x, i, mask)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/src/models/block.py", line 220, in _attention_layer
    attention_output = self.attention_layers[layer_idx](normalized, mask)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/dino/Documents/Projects/ParrotLM/src/models/low_rank_attention.py", line 89, in forward
    torch.bmm(q, k.transpose(1, 2), out=att)
RuntimeError: bmm(): functions with out=... arguments don't support automatic differentiation, but one of the arguments requires grad.
