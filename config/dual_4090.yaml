model:
  dimension: 1536
  n_layers: 16
  n_heads: 24
  rank: 96
  vocab_size: 50257
  max_seq_len: 1024
  dropout: 0.1
  activation: "gelu"

trainer:
  # Dataset configuration
  dataset_name: "roneneldan/TinyStories"
  dataset_split: "train"
  text_column: "text"

  # Training parameters
  train_batch_size: 32 # This will be split across 2 GPUs
  eval_batch_size: 16
  max_length: 1024
  learning_rate: 5e-5
  min_lr: 5e-6
  weight_decay: 0.01
  max_grad_norm: 1.0

  # Optimizer and scheduler
  optimizer: "adamw"
  use_lr_schedule: true
  warmup_steps: 1000

  # Training loop
  max_steps: 50000
  grad_accumulation_steps: 2 # This is handled by DeepSpeed
  eval_every: 500
  save_every: 1000

  # Mixed precision
  use_mixed_precision: true

  # Checkpointing
  resume_from: null
