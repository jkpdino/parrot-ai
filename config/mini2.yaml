model:
  dimension: 192
  heads: 16
  rank: 192
  layers: 1
  max_seq_len: 512
  vocab_size: 50257
  attention_priority: 1
  dropout: 0.1
  bias: true

training:
  optimizer: adamw
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 2000
  gradient_clip: 1.0

  max_steps: 10000
  save_every: 10
  eval_every: 50
  eval_batches: 100
  log_every: 1

  dataset_name: wikitext
  dataset_split: train
  eval_split: validation
  text_column: text
  train_batch_size: 32
  eval_batch_size: 64
  max_length: 512
