model:
  dimension: 2560 # Increased from 1536
  layers: 24 # Increased from 16
  heads: 32 # Increased from 24
  rank: 128 # Increased from 96
  vocab_size: 50257
  max_seq_len: 2048 # Increased from 1024
  dropout: 0.1
  bias: true
  attention_priority: 2

trainer:
  # Dataset configuration
  dataset_name: "roneneldan/TinyStories"
  dataset_split: "train"
  text_column: "text"

  # Training parameters
  train_batch_size: 16 # Reduced from 32 to accommodate larger model
  eval_batch_size: 8 # Reduced from 16
  max_length: 2048 # Increased from 1024
  learning_rate: 3e-5
  min_lr: 3e-6
  weight_decay: 0.01
  max_grad_norm: 1.0
  grad_accumulation_steps: 4 # Increased from 2 for effective batch size

  # Optimizer and scheduler
  optimizer: "adamw"
  use_lr_schedule: true
  warmup_steps: 2000

  # Training loop
  max_steps: 100000
  eval_every: 500
  save_every: 1000

  # Memory optimization settings
  use_mixed_precision: true
  use_gradient_checkpointing: true
  memory_efficient: true

  # Checkpointing
  resume_from: null
